---
layout: article
title: "PRO-GRPO: Positive-Reinforced Optimization for Group Relative Policies"
date: 2026-02-26  
author: "Pengfei Liu"
tags:
  - GRPO
  - LLM
  - Reinforce
mathjax: true 
---

**Abstract**

Reinforcement Learning (RL) has become the cornerstone for unlocking the complex reasoning capabilities of Large Language Models (LLMs). Mainstream alignment algorithms, particularly **GRPO (Group Relative Policy Optimization)**, rely heavily on **Importance Sampling** and **Symmetric Clipping** to constrain policy updates and ensure training stability. However, we argue that this constraint is suboptimal for mathematical and logical reasoning tasks. While strictly constraining negative updates is vital to prevent catastrophic forgetting, clipping positive updates ($A > 0$) inadvertently hinders the model from fully internalizing high-quality, "golden" trajectories.

To address this, we introduce **PRO-GRPO (Positive-Reinforced Optimization for Group Relative Policies)**. PRO-GRPO employs an asymmetric update rule tailored for reasoning: for positive advantages, it completely discards importance sampling and clipping, effectively treating the update as **Advantage-Weighted Supervised Fine-Tuning (AW-SFT)**; for negative advantages, it firmly retains the stable trust-region mechanism of standard GRPO. 

Theoretical analysis reveals that PRO-GRPO inherently resolves the notorious **Train-Inference Mismatch** in Mixture-of-Experts (MoE) models by eliminating the highly volatile importance ratio for positive samples. Empirical results on mathematical reasoning tasks demonstrate that PRO-GRPO achieves significantly faster convergence and higher peak accuracy compared to the standard GRPO baseline, all while maintaining comparable KL divergence stability.

---

## 1. Introduction

The alignment of reasoning LLMs has shifted from standard RLHF to more specialized, efficient algorithms like **GRPO**, popularized by models like DeepSeekMath and DeepSeek-R1. By estimating advantages across a group of sampled outputs rather than relying on a separate critic model, GRPO drastically reduces memory overhead. However, its core policy objective remains heavily rooted in PPO:

$$\mathcal{L}_{policy}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]$$

where $r_t(\theta) = \frac{\pi_\theta}{\pi_{old}}$ is the importance sampling ratio.

While the clipping margin $\epsilon$ is crucial for preventing policy collapse when the model explores poorly ($A < 0$), it introduces a contradictory limitation when the model performs exceptionally well ($A > 0$). In reasoning tasks, the answer space is incredibly sparse. If a model miraculously discovers a complex reasoning chain that yields the correct answer, and the probability of this chain increases significantly ($r_t > 1+\epsilon$), standard GRPO forces the gradient to zero. Effectively, the algorithm tells the model: *"You did brilliantly, but don't learn this too much."*

In this post, we propose **PRO-GRPO**. Our core insight is simple yet profound: **Trust regions are for failure protection, not for limiting success.**

PRO-GRPO partitions the objective based on the sign of the group-relative advantage $A_t$:

1. **When $A_t > 0$:** We strip away the importance ratio $r_t$ and the clipping constraint. The objective beautifully degenerates to minimizing $-\log \pi_\theta$, scaled by $A_t$. Mathematically, this is equivalent to **Advantage-Weighted Supervised Fine-Tuning (AW-SFT)** on the model's own successful rollouts.
2. **When $A_t \le 0$:** We strictly maintain the clipping and trust-region constraints of GRPO to cautiously penalize errors and prevent the policy from aggressively unlearning.

Furthermore, PRO-GRPO offers a massive engineering breakthrough for **Mixture-of-Experts (MoE)** models. In MoE RL, the importance ratio $r_t$ is notoriously unstable. Parameter updates dynamically alter the routing logic, causing a mismatch between the experts activated during rollout ($\pi_{old}$) and training ($\pi_\theta$). By eliminating $r_t$ for positive samples, PRO-GRPO bypasses this routing mismatch entirely, unlocking highly robust updates.

---

## 2. Related Work

**GRPO & Symmetric Constraints**
Standard GRPO revolutionized reasoning RL by replacing the value model with group-based advantage normalization. However, it still inherits PPO's symmetric clipping paradigm [1]. While dual-clip mechanisms exist to prevent overly destructive updates on negative samples, the restriction on positive samples remains largely unquestioned in mainstream setups.



**RL for MoE & Train-Inference Mismatch**
Training MoE models with RL is notoriously difficult due to the **Train-Inference Mismatch**. MoE routing decisions are discontinuous; even microscopic changes in $\theta$ can route tokens to entirely different experts. This renders the importance sampling ratio $r_t = \pi_\theta / \pi_{old}$ mathematically ill-defined and prone to massive variance. PRO-GRPO mitigates this by severing the dependency on $r_t$ for the most crucial (positive) learning signals.

---

## 3. Method: PRO-GRPO Explained

### 3.1 The Objective Function

PRO-GRPO elegantly modifies the standard GRPO policy loss by conditioning the gradient path on the sign of the normalized advantage $A_t$:

$$\mathcal{L}^{PRO-GRPO}(\theta) = \mathbb{E}_{(s,a) \sim \pi_{old}} \begin{cases} \underbrace{- A_t \cdot \log \pi_\theta(a|s)}_{\mathcal{L}_{Pos}} & \text{if } A_t > 0 \\ \underbrace{\mathcal{L}_{GRPO}(r_t(\theta), A_t)}_{\mathcal{L}_{Neg}} & \text{if } A_t \le 0 \end{cases}$$

Where $\mathcal{L}_{GRPO}$ represents the standard clipped surrogate objective (potentially incorporating Dual-Clip) utilized in the baseline.

### 3.2 Theoretical Analysis: Positive Update as Advantage-Weighted SFT

We rigorously prove that for positive advantages, PRO-GRPO seamlessly transitions into Supervised Fine-Tuning (SFT) weighted by the advantage scalar.

**Proof:**
Consider standard SFT on a dataset $\mathcal{D}$: $\mathcal{L}_{SFT} = -\mathbb{E}_{\mathcal{D}}[\log \pi_\theta(a|s)]$. The gradient is simply:
$$\nabla_\theta \mathcal{L}_{SFT} = -\mathbb{E}_{\mathcal{D}} [\nabla_\theta \log \pi_\theta(a|s)]$$

Now, look at the PRO-GRPO positive loss component ($\mathcal{L}_{Pos}$). Here, our "dataset" is the batch of rollouts generated by $\pi_{old}$ where $A_t > 0$. Since $A_t$ is computed during the forward rollout phase, it is a **constant scalar** with respect to the current policy parameters $\theta$ during the backward pass. The gradient becomes:

$$\nabla_\theta \mathcal{L}_{Pos} = \nabla_\theta (- A_t \log \pi_\theta(a|s)) = - A_t \cdot \nabla_\theta \log \pi_\theta(a|s)$$

**Key Observations:**
1. **Directional Purity:** The gradient direction precisely mirrors SFT. It directly maximizes the likelihood of the golden tokens.
2. **Proportional Magnitude:** Unlike standard SFT (where weight=1), PRO-GRPO scales the gradient by $A_t$. Higher-quality reasoning paths trigger stronger updates.
3. **Independence from $\pi_{old}$:** Crucially, $r_t(\theta)$ is absent. The update no longer cares "how far" $\pi_\theta$ has drifted from $\pi_{old}$ during the PPO epochs; it only cares that $\pi_\theta$ assigns maximum probability to the correct action.

### 3.3 Solving the MoE Routing Mismatch

In an MoE architecture, the token probability $\pi_\theta(a|s)$ is a weighted sum determined by a gating network $g(s)$:
$$\pi_\theta(a|s) = \sum_{i} g_i(s) \cdot E_i(a|s)$$

In standard GRPO, the ratio $r_t = \frac{\pi_\theta}{\pi_{old}}$ attempts to compare the new probability to the old. If an update to $\theta$ shifts the gating $g(s)$ from Expert 1 to Expert 5, the numerator and denominator now originate from vastly different distributions. $r_t$ fluctuates wildly, destabilizing the gradient.

**The PRO-GRPO Fix:** By switching to $-A_t \log \pi_\theta$ for positive samples, we **completely remove $\pi_{old}$ from the equation**. We are simply instructing the *current* MoE router to maximize the probability of the correct token. If the router decides a new expert is better suited than the one used during the old rollout, PRO-GRPO allows it seamlessly. This makes PRO-GRPO inherently robust for MoE training.

---

## 4. Experiments

We evaluated PRO-GRPO against a standard DAPO baseline on a mathematical reasoning task using open-source data.

* **Baseline:** Standard DAPO.
* **Model:** Qwen3-14B-base.
* **Setup:** Both runs use identical datasets, group size ($G=64$), learning rates, and KL penalties. The **only** variable is the policy loss function.

### 4.1 Results Analysis

**1. Accuracy (Acc): Breaking the Ceiling**
As shown in **Figure 1 (Acc Curve)**, both PRO-GRPO (light red) and Baseline GRPO (green) track together during the initial warmup. However:

!['Acc Curve']({{ site.baseurl }}/output_7.png)
*Figure 1: Accuracy (Acc) Curve*

* **Peak Performance:** PRO-GRPO reaches a peak accuracy of **0.5114** (Step 120), whereas the GRPO Baseline plateaus early at **0.4770**. 
Removing the positive clip clearly allows the model to escape suboptimal local minima and confidently memorize correct mathematical reasoning patterns.

**2. KL Divergence: Surprisingly Stable**
A common theoretical concern is that unclipped updates will cause the policy to drift uncontrollably (spiking KL divergence). However, **Figure 2 (KL Curve)** shows PRO-GRPO's KL divergence remains completely stable, often sitting slightly *lower* than the baseline.

!['KL Curve']({{ site.baseurl }}/output_6.png)
*Figure 2: KL Divergence Curve*

* *Why?* Because we strictly enforce the clip/trust-region for negative advantages ($A_t \le 0$), "bad" explorations are still heavily constrained. "Good" explorations ($A_t > 0$), while unclipped, naturally pull the model toward high-reward, fundamentally correct regions of the landscape, preventing chaotic drift.

**3. Entropy & Early Stopping**
**Figure 3 (Entropy Curve)** reveals a striking differences between PRO-GRPO and standard GRPO on their policy entropy curves. While standard GRPO tends to plateau at a lower entropy level in the later stages of training, PRO-GRPO exhibits a significant and sustained surge. 

!['Entropy Curve']({{ site.baseurl }}/output_5.png)
*Figure 3: Policy Entropy Curve*

The secret behind this 'entropy boost' is PRO-GRPO's elegant asymmetric loss design. By removing the clipping constraint strictly for positive advantages ($A > 0$), PRO-GRPO allows the model to aggressively update its policy whenever it discovers exceptionally high-quality, novel responses. Standard GRPO artificially caps the reward for these good explorations, which often forces the model into a narrow probability distribution—a phenomenon known as 'mode collapse'. In contrast, PRO-GRPO’s unbounded positive reinforcement encourages the model to distribute its probability mass across a wider variety of excellent tokens. For the end user, this higher entropy translates to a language model that is not only highly aligned but also remarkably diverse, creative, and resistant to repetitive, formulaic outputs.

---

## 5. Conclusion

**PRO-GRPO** is a minimalist yet incredibly powerful algorithmic upgrade for reasoning models. By mathematically restructuring positive advantages into **Advantage-Weighted SFT**—while retaining standard GRPO trust regions for negative advantages—PRO-GRPO perfectly aligns with human learning intuition: *reinforce success confidently, but correct failure cautiously.*

Our experiments prove that PRO-GRPO not only outperforms standard GRPO in mathematical reasoning accuracy but also serves as an elegant theoretical fix for the train-inference routing mismatch that plagues MoE architectures.

--

